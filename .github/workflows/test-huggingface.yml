name: Huggingface TGI integration test

on:
  workflow_dispatch:
    inputs:
      tgi-version:
        description: 'tgi version'
        required: true
        default: '0.8.2'
      pytorch-version:
        description: 'pytorch version'
        required: true
        default: '2.0.0'
      cuda-version:
        description: 'cuda version'
        required: true
        default: '118'
      ubuntu-version:
        description: 'ubuntu version'
        required: true
        default: '20.04'

jobs:
  create-runners:
    runs-on: [self-hosted, scheduler]
    steps:
      - name: Create new G5 instance
        id: create_gpu
        run: |
          cd /home/ubuntu/djl_benchmark_script/scripts
          token=$( curl -X POST -H "Authorization: token ${{ secrets.ACTION_RUNNER_PERSONAL_TOKEN }}" \
          https://api.github.com/repos/awslabs/llm-hosting-container/actions/runners/registration-token \
          --fail \
          | jq '.token' | tr -d '"' )
          ./start_instance.sh action_g5 $token awslabs/llm-hosting-container
    outputs:
      gpu_instance_id: ${{ steps.create_gpu.outputs.action_g5_instance_id }}

  run-tests:
    runs-on: [ self-hosted, g5 ]
    timeout-minutes: 30
    needs: create-runners
    env:
      TGI_VERSION: ${{github.event.inputs.tgi-version}}
      REPOSITORY: djl-serving
      TAG: ${{github.event.inputs.pytorch-version}}-tgi${{github.event.inputs.tgi-version}}-gpu-py39-cu${{github.event.inputs.cuda-version}}-ubuntu${{github.event.inputs.ubuntu-version}}
    steps:
      - uses: actions/checkout@v3
      - name: Clean env
        run: |
          yes | docker system prune -a --volumes
          sudo rm -rf /home/ubuntu/actions-runner/_work/_tool/Java_Corretto_jdk/
          echo "wait dpkg lock..."
          while sudo fuser /var/{lib/{dpkg,apt/lists},cache/apt/archives}/lock >/dev/null 2>&1; do sleep 5; done
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-region: us-east-1
      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v1
        with:
          registries: "125045733377"
      - name: Pull docker
        env:
          REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        run: |
          docker pull ${REGISTRY}/${REPOSITORY}:${TAG}
      - name: Test bloom-560m
        env:
          REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        run: |
          set -ex
          HF_MODEL_ID=bigscience/bloom-560m && \
          SM_NUM_GPUS=4 && \
          TGI_VERSION=$TGI_VERSION && \
          docker run --gpus all --shm-size 2g -itd --rm -p 8080:8080 \
              -e SM_NUM_GPUS=$SM_NUM_GPUS -e HF_MODEL_ID=$HF_MODEL_ID \
              ${REGISTRY}/${REPOSITORY}:${TAG}
          sleep 30
          ret=$(curl http://localhost:8080/invocations -X POST \
              -d '{"inputs":"What is Deep Learning?","parameters":{"max_new_tokens":128}}' \
              -H 'Content-Type: application/json')
          [[ $ret != "[{\"generated_text\":\"What is Deep Learning?"* ]] && exit 1
          docker rm -f $(docker ps -aq)
      - name: Test gpt-neox-20b
        env:
          REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        run: |
          set -ex
          HF_MODEL_ID=EleutherAI/gpt-neox-20b && \
          SM_NUM_GPUS=4 && \
          TGI_VERSION=$TGI_VERSION && \
          docker run --gpus all --shm-size 2g -itd --rm -p 8080:8080 \
              -e SM_NUM_GPUS=$SM_NUM_GPUS -e HF_MODEL_ID=$HF_MODEL_ID \
              ${REGISTRY}/${REPOSITORY}:${TAG}
          sleep 150
          ret=$(curl http://localhost:8080/invocations -X POST \
              -d '{"inputs":"What is Deep Learning?","parameters":{"max_new_tokens":128}}' \
              -H 'Content-Type: application/json')
          [[ $ret != "[{\"generated_text\":\"What is Deep Learning?"* ]] && exit 1
          docker rm -f $(docker ps -aq)
      - name: Test flan-t5-xxl
        env:
          REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        run: |
          set -ex
          HF_MODEL_ID=google/flan-t5-xxl && \
          SM_NUM_GPUS=4 && \
          TGI_VERSION=$TGI_VERSION && \
          docker run --gpus all --shm-size 2g -itd --rm -p 8080:8080 \
              -e SM_NUM_GPUS=$SM_NUM_GPUS -e HF_MODEL_ID=$HF_MODEL_ID \
              ${REGISTRY}/${REPOSITORY}:${TAG}
          sleep 300
          ret=$(curl http://localhost:8080/invocations -X POST \
              -d '{"inputs":"What is Deep Learning?","parameters":{"max_new_tokens":128}}' \
              -H 'Content-Type: application/json')
          [[ $ret != "[{\"generated_text\""* ]] && exit 1
          docker rm -f $(docker ps -aq)
      - name: On fail step
        if: ${{ failure() }}
        run: |
          docker rm -f $(docker ps -aq) || true

  stop-runners:
    if: always()
    runs-on: [ self-hosted, scheduler ]
    needs: [ create-runners, run-tests ]
    steps:
      - name: Stop all instances
        run: |
          cd /home/ubuntu/djl_benchmark_script/scripts
          instance_id=${{ needs.create-runners.outputs.gpu_instance_id }}
          ./stop_instance.sh $instance_id